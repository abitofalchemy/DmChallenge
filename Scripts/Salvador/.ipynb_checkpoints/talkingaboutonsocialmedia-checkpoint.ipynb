{
 "metadata": {
  "name": "",
  "signature": "sha256:004cdf60d517599ac442e45ea8fbe923c3e8dec9b3b30914a46f1acaa20ef418"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Crawl Twitter for Tweets</h2>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Enhancing Tweets with **text** from tweet's urls </h2>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from D_GetTweetsWithUrls import parse_sci_tweets\n",
      "from D_GetTweetsWithUrls import filter_tweets\n",
      "from D_GetTweetsWithUrls import getTweetsWithUrls\n",
      "from D_GetTweetsWithUrls import processTweet\n",
      "\n",
      "from newspaper import Article\n",
      "\n",
      "## Get enginlish tweets parsed\n",
      "tweets_dict = parse_sci_tweets(\"../../Data_Schurz/toy.json\")\n",
      "## Filter tweets (trim the tweet)\n",
      "filt_t_dict = filter_tweets(tweets_dict)\n",
      "\n",
      "## Variable initialization\n",
      "min_tweet_dict = dict()\n",
      "enhanced_cleaned_tw_lst = []\n",
      "\n",
      "## Get tweets with urls\n",
      "for tweet in filt_t_dict:\n",
      "    tstStr = getTweetsWithUrls(filt_t_dict[tweet][0])\n",
      "    if (tstStr) is not None:\n",
      "        min_tweet_dict[tweet] =  [tstStr,filt_t_dict[tweet][0]] #(getTweetsWithUrls(filt_t_dict[tweet][0])\n",
      "\n",
      "## Further Cleaning/Enhancing the orig tweets\n",
      "cnt = 0\n",
      "for urlTweet in min_tweet_dict:\n",
      "    \n",
      "    url = min_tweet_dict[urlTweet][0]\n",
      "    a = Article(url, language='en')\n",
      "    a.download()\n",
      "    a.parse()\n",
      "    #print \"Ehriched tweet:\\n\\t\"+(min_tweet_dict[urlTweet][1]+\",\"+a.text)\n",
      "    #if a.text() is not None:\n",
      "    try:\n",
      "        url_augmented_tweet= (\"%s %s\")%(min_tweet_dict[urlTweet][1],a.title.decode(\"utf-8\"))\n",
      "    except: \n",
      "      print cnt\n",
      "      print min_tweet_dict[urlTweet][1],a.title\n",
      "    enhanced_cleaned_tw_lst.append(processTweet(url_augmented_tweet))\n",
      "    cnt += 1\n",
      "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "print \"Raw tweet\\n\\t\",min_tweet_dict.itervalues().next()  \n",
      "print \"Cleaned and augmented tweet\\n\\t\",enhanced_cleaned_tw_lst[0]\n",
      "print \"Tweets cleaned/augmented tweets: \",len(enhanced_cleaned_tw_lst)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "CRITICAL:newspaper.article:jpeg error with PIL, cannot concatenate 'str' and 'NoneType' objects\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "91 English tweets processed\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "RT @WIRED: Big, powerful, and cheap, the OnePlus One is one of the best deals in smartphones. Review: http://t.co/D8uby1W7mA http://t.co/bU\u2026 Review: OnePlus One\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "CRITICAL:newspaper.article:jpeg error with PIL, cannot concatenate 'str' and 'NoneType' objects\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "@Saugatuck \u25bc Interesting blog about traveling in africa on the road - Have a look! \u2660 http://t.co/CSmkF0es4E \u25bc A Journey from Europe to Cape Town\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\u266b Welcome to Miami \u2013 Buenvenidos a Miami!..\u266b Hot Miami food deals http://t.co/1GDINscv03 #miami Miami Daily Deals \u00b7 GreedyHogs\n",
        "[Parse lxml ERR]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " line 2055: htmlParseEntityRef: no name\n",
        "[Article parse ERR] http://t.co/YRZxhMcTCb\n",
        "[Parse lxml ERR] line 2055: htmlParseEntityRef: no name\n",
        "[Article parse ERR] http://t.co/uHpflTNi57\n",
        "[Parse lxml ERR] line 2055: htmlParseEntityRef: no name\n",
        "[Article parse ERR] http://t.co/uHpflTNi57\n",
        "Raw tweet\n",
        "\t['http://t.co/SJ0DINA13C ', '.@itcdm sucks when u take the weekend off. Doin work today: Cardio/Core day! I just ran 2.01 mi with Nike+. http://t.co/SJ0DINA13C #nikeplus']\n",
        "Cleaned and augmented tweet\n",
        "\t.AT_USER sucks when u take the weekend off. doin work today: cardio/core day! i just ran 2.01 mi with nike+. URL nikeplus \n",
        "Tweets cleaned/augmented tweets:  18\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Above **Cleaned and augmented tweet** comes from using the Article module.  Next, using Beautifulsoup:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## NB: this section below does not product workable output\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "\n",
      "for urlTweet in min_tweet_dict:\n",
      "    url = min_tweet_dict[urlTweet][0]\n",
      "    #print \"Raw tweet\\n\\t\",min_tweet_dict[urlTweet][1]\n",
      "    data=urllib2.urlopen(url).read()\n",
      "    #parser = MyHTMLParser()\n",
      "    soup = BeautifulSoup(data)\n",
      "    #print(soup.get_text())\n",
      "    url_augmented_tweet= min_tweet_dict[urlTweet][1]+\",\"+soup.get_text()\n",
      "    #print \"Cleaned and augmented tweet\\n\\t\",processTweet(url_augmented_tweet)\n",
      "    break\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Raw tweet\n",
        "\t.@itcdm sucks when u take the weekend off. Doin work today: Cardio/Core day! I just ran 2.01 mi with Nike+. http://t.co/SJ0DINA13C #nikeplus\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "No, using Beautifulsoup, did not work well."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Enriched Tweets: using external public sources</h3>\n",
      "This section will generate a corpus for each tweet.  Q? How do we section the corpus? In other words, should  the corpus be for a given follower, date/time range, quartiles of the most prolific tweeters?\n",
      "For example, the following is for a corpus from (100) tweets:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print enhanced_cleaned_tw_lst[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'.AT_USER sucks when u take the weekend off. doin work today: cardio/core day! i just ran 2.01 mi with nike+. URL nikeplus ', u'rt AT_USER ndgain2014 index with latest data and analytics bigdata for common good URL ndresearch via AT_USER nd-gain index', u\"i have completed the quest 'another try' in the android game the tribez. URL androidgames, gameinsight game insight\"]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Working with Enhanced Tweets**\n",
      "Systematic trying out different NLP tools:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "for sentence in enhanced_cleaned_tw_lst:\n",
      "    tokens = nltk.word_tokenize(sentence)\n",
      "    tagged = nltk.pos_tag(tokens)\n",
      "    ## identify the named entities:\n",
      "    entities = nltk.chunk.ne_chunk (tagged)\n",
      "    print entities\n",
      "    break\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'module' object has no attribute 'ne_check'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-4-3859a4f215ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m## identify the named entities:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'ne_check'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}