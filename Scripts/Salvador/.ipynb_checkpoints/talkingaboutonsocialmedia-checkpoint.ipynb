{
 "metadata": {
  "name": "",
  "signature": "sha256:b0dfc72a586f86992547fc25aae236297e72230d72b58ae684c933d9a6b1b954"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Crawl Twitter for Tweets</h2>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Enhancing Tweets with **text** from tweet's urls </h2>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from D_GetTweetsWithUrls import parse_sci_tweets\n",
      "from D_GetTweetsWithUrls import filter_tweets\n",
      "from D_GetTweetsWithUrls import getTweetsWithUrls\n",
      "from D_GetTweetsWithUrls import processTweet\n",
      "\n",
      "from newspaper import Article\n",
      "\n",
      "## Get enginlish tweets parsed\n",
      "tweets_dict = parse_sci_tweets(\"../../Data_Schurz/toy.json\")\n",
      "## Filter tweets (trim the tweet)\n",
      "filt_t_dict = filter_tweets(tweets_dict)\n",
      "\n",
      "## Get tweets with urls\n",
      "min_tweet_dict = dict()\n",
      "for tweet in filt_t_dict:\n",
      "    tstStr = getTweetsWithUrls(filt_t_dict[tweet][0])\n",
      "    if (tstStr) is not None:\n",
      "        min_tweet_dict[tweet] =  [tstStr,filt_t_dict[tweet][0]] #(getTweetsWithUrls(filt_t_dict[tweet][0])\n",
      "## Further Cleaning/Enhancing the orig tweets\n",
      "for urlTweet in min_tweet_dict:\n",
      "    url = min_tweet_dict[urlTweet][0]\n",
      "    print \"Raw tweet\\n\\t\",min_tweet_dict[urlTweet][1]\n",
      "    a = Article(url, language='en')\n",
      "    a.download()\n",
      "    a.parse()\n",
      "    #print \"Ehriched tweet:\\n\\t\"+(min_tweet_dict[urlTweet][1]+\",\"+a.text)\n",
      "    url_augmented_tweet= min_tweet_dict[urlTweet][1]+\",\"+a.text\n",
      "    print \"Cleaned and augmented tweet\\n\\t\",processTweet(url_augmented_tweet)\n",
      "    break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "91 English tweets processed\n",
        "Raw tweet\n",
        "\t.@itcdm sucks when u take the weekend off. Doin work today: Cardio/Core day! I just ran 2.01 mi with Nike+. http://t.co/SJ0DINA13C #nikeplus\n",
        "Cleaned and augmented tweet\n",
        "\t"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".AT_USER sucks when u take the weekend off. doin work today: cardio/core day! i just ran 2.01 mi with nike+. URL nikeplus,inspiration and tools to help you run your best.\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Above **Cleaned and augmented tweet** comes from using the Article module.  Next, using Beautifulsoup:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "\n",
      "for urlTweet in min_tweet_dict:\n",
      "    url = min_tweet_dict[urlTweet][0]\n",
      "    print \"Raw tweet\\n\\t\",min_tweet_dict[urlTweet][1]\n",
      "    data=urllib2.urlopen(url).read()\n",
      "    #parser = MyHTMLParser()\n",
      "    soup = BeautifulSoup(data)\n",
      "    #print(soup.get_text())\n",
      "    url_augmented_tweet= min_tweet_dict[urlTweet][1]+\",\"+soup.get_text()\n",
      "    #print \"Cleaned and augmented tweet\\n\\t\",processTweet(url_augmented_tweet)\n",
      "    break\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Raw tweet\n",
        "\t.@itcdm sucks when u take the weekend off. Doin work today: Cardio/Core day! I just ran 2.01 mi with Nike+. http://t.co/SJ0DINA13C #nikeplus\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "No, using Beautifulsoup, did not work well."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Enriched Tweets: using external public sources</h3>\n",
      "This section will generate a corpus for each tweet.  Q? How do we section the corpus? In other words, should  the corpus be for a given follower, date/time range, quartiles of the most prolific tweeters?\n",
      "For example, the following is for a corpus from (100) tweets:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "augmented_tweets = [] # list \n",
      "for urlTweet in min_tweet_dict:\n",
      "    url = min_tweet_dict[urlTweet][0]\n",
      "    #print \"Raw tweet\\n\\t\",min_tweet_dict[urlTweet][1]\n",
      "    a = Article(url, language='en')\n",
      "    a.download()\n",
      "    a.parse()\n",
      "    #print \"Ehriched tweet:\\n\\t\"+(min_tweet_dict[urlTweet][1]+\",\"+a.text)\n",
      "    url_augmented_tweet= min_tweet_dict[urlTweet][1].decode(\"utf-8\")+\",\"+a.text.decode(\"utf-8\")\n",
      "    augmented_tweets.append(processTweet(url_augmented_tweet))\n",
      "\n",
      "print augmented_tweets[:3]    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "CRITICAL:newspaper.article:jpeg error with PIL, cannot concatenate 'str' and 'NoneType' objects\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "CRITICAL:newspaper.article:jpeg error with PIL, cannot concatenate 'str' and 'NoneType' objects\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'.AT_USER sucks when u take the weekend off. doin work today: cardio/core day! i just ran 2.01 mi with nike+. URL nikeplus', u'rt AT_USER ndgain2014 index with latest data and analytics bigdata for common good URL ndresearch via AT_USER', u\"i have completed the quest 'another try' in the android game the tribez. URL androidgames, gameinsight\"]\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}