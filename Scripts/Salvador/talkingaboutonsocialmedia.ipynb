{
 "metadata": {
  "name": "",
  "signature": "sha256:c01098bf79fcd7214fa9d155afbb3cb50f474db9356540e894ef15d3e6afc312"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Crawl Twitter for Tweets</h2>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h2>Enhancing Tweets with **text** from tweet's urls </h2>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from D_GetTweetsWithUrls import parse_sci_tweets\n",
      "from D_GetTweetsWithUrls import filter_tweets\n",
      "from D_GetTweetsWithUrls import getTweetsWithUrls\n",
      "from D_GetTweetsWithUrls import processTweet\n",
      "\n",
      "from newspaper import Article\n",
      "\n",
      "## Get enginlish tweets parsed\n",
      "tweets_dict = parse_sci_tweets(\"../../Data_Schurz/toy.json\")\n",
      "## Filter tweets (trim the tweet)\n",
      "filt_t_dict = filter_tweets(tweets_dict)\n",
      "\n",
      "## Variable initialization\n",
      "min_tweet_dict = dict()\n",
      "enhanced_cleaned_tw_lst = []\n",
      "\n",
      "## Get tweets with urls\n",
      "for tweet in filt_t_dict:\n",
      "    tstStr = getTweetsWithUrls(filt_t_dict[tweet][0])\n",
      "    if (tstStr) is not None:\n",
      "        min_tweet_dict[tweet] =  [tstStr,filt_t_dict[tweet][0]] #(getTweetsWithUrls(filt_t_dict[tweet][0])\n",
      "\n",
      "## Further Cleaning/Enhancing the orig tweets\n",
      "cnt = 0\n",
      "for urlTweet in min_tweet_dict:\n",
      "    \n",
      "    url = min_tweet_dict[urlTweet][0]\n",
      "    a = Article(url, language='en')\n",
      "    a.download()\n",
      "    a.parse()\n",
      "    #print \"Ehriched tweet:\\n\\t\"+(min_tweet_dict[urlTweet][1]+\",\"+a.text)\n",
      "    #if a.text() is not None:\n",
      "    try:\n",
      "        url_augmented_tweet= (\"%s %s\")%(min_tweet_dict[urlTweet][1],a.title.decode(\"utf-8\"))\n",
      "    except: \n",
      "      print cnt\n",
      "      print min_tweet_dict[urlTweet][1],a.title\n",
      "    enhanced_cleaned_tw_lst.append(processTweet(url_augmented_tweet))\n",
      "    cnt += 1\n",
      "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "print \"Raw tweet\\n\\t\",min_tweet_dict.itervalues().next()  \n",
      "print \"Cleaned and augmented tweet\\n\\t\",enhanced_cleaned_tw_lst[0]\n",
      "print \"Tweets cleaned/augmented tweets: \",len(enhanced_cleaned_tw_lst)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "CRITICAL:newspaper.article:jpeg error with PIL, cannot concatenate 'str' and 'NoneType' objects\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "91 English tweets processed\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "RT @WIRED: Big, powerful, and cheap, the OnePlus One is one of the best deals in smartphones. Review: http://t.co/D8uby1W7mA http://t.co/bU\u2026 Review: OnePlus One\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "CRITICAL:newspaper.article:jpeg error with PIL, cannot concatenate 'str' and 'NoneType' objects\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "@Saugatuck \u25bc Interesting blog about traveling in africa on the road - Have a look! \u2660 http://t.co/CSmkF0es4E \u25bc A Journey from Europe to Cape Town\n",
        "14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\u266b Welcome to Miami \u2013 Buenvenidos a Miami!..\u266b Hot Miami food deals http://t.co/1GDINscv03 #miami Miami Daily Deals \u00b7 GreedyHogs\n",
        "[Parse lxml ERR]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " line 2055: htmlParseEntityRef: no name\n",
        "[Article parse ERR] http://t.co/YRZxhMcTCb\n",
        "[Parse lxml ERR] line 2055: htmlParseEntityRef: no name\n",
        "[Article parse ERR] http://t.co/uHpflTNi57\n",
        "[Parse lxml ERR] line 2055: htmlParseEntityRef: no name\n",
        "[Article parse ERR] http://t.co/uHpflTNi57\n",
        "Raw tweet\n",
        "\t['http://t.co/SJ0DINA13C ', '.@itcdm sucks when u take the weekend off. Doin work today: Cardio/Core day! I just ran 2.01 mi with Nike+. http://t.co/SJ0DINA13C #nikeplus']\n",
        "Cleaned and augmented tweet\n",
        "\t.AT_USER sucks when u take the weekend off. doin work today: cardio/core day! i just ran 2.01 mi with nike+. URL nikeplus \n",
        "Tweets cleaned/augmented tweets:  18\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Above **Cleaned and augmented tweet** comes from using the Article module.  Next, using Beautifulsoup:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## NB: this section below does not product workable output\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "\n",
      "for urlTweet in min_tweet_dict:\n",
      "    url = min_tweet_dict[urlTweet][0]\n",
      "    #print \"Raw tweet\\n\\t\",min_tweet_dict[urlTweet][1]\n",
      "    data=urllib2.urlopen(url).read()\n",
      "    #parser = MyHTMLParser()\n",
      "    soup = BeautifulSoup(data)\n",
      "    #print(soup.get_text())\n",
      "    url_augmented_tweet= min_tweet_dict[urlTweet][1]+\",\"+soup.get_text()\n",
      "    #print \"Cleaned and augmented tweet\\n\\t\",processTweet(url_augmented_tweet)\n",
      "    break\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Raw tweet\n",
        "\t.@itcdm sucks when u take the weekend off. Doin work today: Cardio/Core day! I just ran 2.01 mi with Nike+. http://t.co/SJ0DINA13C #nikeplus\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "No, using Beautifulsoup, did not work well."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Enriched Tweets: using external public sources</h3>\n",
      "This section will generate a corpus for each tweet.  Q? How do we section the corpus? In other words, should  the corpus be for a given follower, date/time range, quartiles of the most prolific tweeters?\n",
      "For example, the following is for a corpus from (100) tweets:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pprint \n",
      "pprint.pprint (enhanced_cleaned_tw_lst[:3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'.AT_USER sucks when u take the weekend off. doin work today: cardio/core day! i just ran 2.01 mi with nike+. URL nikeplus ',\n",
        " u'rt AT_USER ndgain2014 index with latest data and analytics bigdata for common good URL ndresearch via AT_USER nd-gain index',\n",
        " u\"i have completed the quest 'another try' in the android game the tribez. URL androidgames, gameinsight game insight\"]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Working with Enhanced Tweets**\n",
      "Systematic trying out different NLP tools:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "for sentence in enhanced_cleaned_tw_lst:\n",
      "    tokens = nltk.word_tokenize(sentence)\n",
      "    tagged = nltk.pos_tag(tokens)\n",
      "    ## identify the named entities:\n",
      "    entities = nltk.chunk.ne_chunk (tagged)\n",
      "    print entities\n",
      "    break\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "LookupError",
       "evalue": "\n**********************************************************************\n  Resource\n  u'chunkers/maxent_ne_chunker/english_ace_multiclass.pickle' not\n  found.  Please use the NLTK Downloader to obtain the resource:\n  >>> nltk.download()\n  Searched in:\n    - '/Users/chavo/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - u''\n**********************************************************************",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-e4cc82fea50d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m## identify the named entities:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne_chunk\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/chunk/__init__.pyc\u001b[0m in \u001b[0;36mne_chunk\u001b[0;34m(tagged_tokens, binary)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mchunker_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MULTICLASS_NE_CHUNKER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunker_pickle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource\n  u'chunkers/maxent_ne_chunker/english_ace_multiclass.pickle' not\n  found.  Please use the NLTK Downloader to obtain the resource:\n  >>> nltk.download()\n  Searched in:\n    - '/Users/chavo/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - u''\n**********************************************************************"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}